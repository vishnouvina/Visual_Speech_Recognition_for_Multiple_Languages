{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89tk8KwgJIEX"
      },
      "source": [
        "# Auto-AVSR Tutorial\n",
        "**Authors**: [Pingchuan Ma](https://mpc001.github.io/), [Alexandros Haliassos](https://dblp.org/pid/257/3052.html), [Adriana Fernandez-Lopez](https://scholar.google.com/citations?user=DiVeQHkAAAAJ), [Honglie Chen](https://scholar.google.com/citations?user=HPwdvwEAAAAJ), [Stavros Petridis](https://ibug.doc.ic.ac.uk/people/spetridis), [Maja Pantic](https://ibug.doc.ic.ac.uk/people/mpantic).\n",
        "\n",
        "This tutorial shows how to use Auto-AVSR model to perform speech recognition (ASR, VSR, and AV-ASR), crop mouth ROIs or extract visual speech features.\n",
        "\n",
        "**Disclaimer**: Please note that both the VSR model and AV-ASR model have been trained with videos that were pre-processed by RetinaFace. For the purpose of improving inference speed, we use mediapipe instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!git clone https://github.com/mpc001/Visual_Speech_Recognition_for_Multiple_Languages.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRR0bdqNLXTc",
        "outputId": "b28cd42d-3867-495c-dea4-4f1a925a8338"
      },
      "outputs": [],
      "source": [
        "#!pip install torch torchvision torchaudio\n",
        "#!pip install opencv-python\n",
        "#!pip install scipy\n",
        "#!pip install scikit-image\n",
        "#!pip install av\n",
        "#!pip install six\n",
        "#!pip install mediapipe\n",
        "#!pip install ffmpeg-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from pipelines.model import AVSR\n",
        "from pipelines.data.data_module import AVSRDataLoader\n",
        "from pipelines.detectors.mediapipe.detector import LandmarksDetector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1nHlTL5OHBh"
      },
      "source": [
        "## Building an inference pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9kfb4RFIOYTk"
      },
      "outputs": [],
      "source": [
        "class InferencePipeline(torch.nn.Module):\n",
        "    def __init__(self, modality, model_path, model_conf, detector=\"mediapipe\", face_track=False, device=\"cpu\"):\n",
        "        super(InferencePipeline, self).__init__()\n",
        "        self.device = device\n",
        "        # modality configuration\n",
        "        self.modality = modality\n",
        "        self.dataloader = AVSRDataLoader(modality, detector=detector)\n",
        "        self.model = AVSR(modality, model_path, model_conf, rnnlm=None, rnnlm_conf=None, penalty=0.0, ctc_weight=0.1, lm_weight=0.0, beam_size=40, device=device)\n",
        "        if face_track and self.modality in [\"video\", \"audiovisual\"]:\n",
        "            self.landmarks_detector = LandmarksDetector()\n",
        "        else:\n",
        "            self.landmarks_detector = None\n",
        "\n",
        "\n",
        "    def process_landmarks(self, data_filename, landmarks_filename):\n",
        "        if self.modality == \"audio\":\n",
        "            return None\n",
        "        if self.modality in [\"video\", \"audiovisual\"]:\n",
        "            landmarks = self.landmarks_detector(data_filename)\n",
        "            return landmarks\n",
        "\n",
        "\n",
        "    def forward(self, data_filename, landmarks_filename=None):\n",
        "        assert os.path.isfile(data_filename), f\"data_filename: {data_filename} does not exist.\"\n",
        "        landmarks = self.process_landmarks(data_filename, landmarks_filename)\n",
        "        data = self.dataloader.load_data(data_filename, landmarks)\n",
        "        transcript = self.model.infer(data)\n",
        "        return transcript\n",
        "\n",
        "    def extract_features(self, data_filename, landmarks_filename=None, extract_resnet_feats=False):\n",
        "        assert os.path.isfile(data_filename), f\"data_filename: {data_filename} does not exist.\"\n",
        "        landmarks = self.process_landmarks(data_filename, landmarks_filename)\n",
        "        data = self.dataloader.load_data(data_filename, landmarks)\n",
        "        with torch.no_grad():\n",
        "            if isinstance(data, tuple):\n",
        "                enc_feats = self.model.model.encode(data[0].to(self.device), data[1].to(self.device), extract_resnet_feats)\n",
        "            else:\n",
        "                enc_feats = self.model.model.encode(data.to(self.device), extract_resnet_feats)\n",
        "        return enc_feats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kFGHsi3gx7i"
      },
      "source": [
        "## Auto-AVSR functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPKcjH5bjtgZ"
      },
      "source": [
        "### Infer the noisy clip using a video stream\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBLqkRWnkA1u"
      },
      "source": [
        "1. Download a VSR checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6S5H_Wuej3tq"
      },
      "outputs": [],
      "source": [
        "#https://huggingface.co/quakumei/Visual_Speech_Recognition_for_Multiple_Languages/tree/main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtfZTpWNlR33"
      },
      "source": [
        "2. Build a VSR pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4l3pLd5YjyuZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1761408717.715261 22169651 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4 Max\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "I0000 00:00:1761408717.718799 22169651 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4 Max\n",
            "W0000 00:00:1761408717.718991 22172761 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W0000 00:00:1761408717.729809 22172773 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
          ]
        }
      ],
      "source": [
        "modality = \"video\"\n",
        "model_conf = \"../models/LRS3_V_WER19.1/model.json\"\n",
        "model_path = \"../models/LRS3_V_WER19.1/model.pth\"\n",
        "pipeline = InferencePipeline(modality, model_path, model_conf, face_track=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbbX9m1YktvP"
      },
      "source": [
        "3. Infer the noisy clip using the video stream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HeWC64v3k0au"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/vishnou/Documents/vsr/Visual_Speech_Recognition_for_Multiple_Languages/.vsr/lib/python3.11/site-packages/torchvision/io/_video_deprecation_warning.py:9: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TO YOU SHE WAS YOUR QUEEN TO US SHE WAS THE QUEEN TO US ALL SHE WOULD BE WITH US FOREVER\n"
          ]
        }
      ],
      "source": [
        "transcript = pipeline(\"/Users/vishnou/Documents/echo-charlie/data/videos/macron_1.mp4\")\n",
        "print(transcript)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ1mQd60pXKn"
      },
      "source": [
        "### Infer the noisy clip using both audio and visual streams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxL6y0_epgdL"
      },
      "source": [
        "1. Download a AV-ASR checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgEakNr8piLH",
        "outputId": "76855e4b-62e3-4705-86e3-739bada5f033"
      },
      "outputs": [],
      "source": [
        "#https://huggingface.co/quakumei/Visual_Speech_Recognition_for_Multiple_Languages/tree/main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntXhBF74pp26"
      },
      "source": [
        "2. Build an AV-ASR pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2o5KiQVpu5d"
      },
      "outputs": [],
      "source": [
        "modality = \"audiovisual\"\n",
        "model_conf = \"../models/LRS3_AV_WER0.9/model.json\"\n",
        "model_path = \"../models/LRS3_AV_WER0.9/model.pth\"\n",
        "pipeline = InferencePipeline(modality, model_path, model_conf, face_track=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PO-F0hTp2cn"
      },
      "source": [
        "3. Infer the noisy clip using both audio and video streams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_7Pl6Pdp5Nt",
        "outputId": "37b578de-0beb-4b44-84ef-33bf3dffbf8e"
      },
      "outputs": [],
      "source": [
        "transcript = pipeline(\"../data/noisy_clip.mp4\")\n",
        "print(transcript)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWPJ1iD0zB-M"
      },
      "source": [
        "### Crop mouth ROIs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJ_MZoIKza5m"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torchvision\n",
        "from pipelines.data.data_module import AVSRDataLoader\n",
        "from pipelines.detectors.mediapipe.detector import LandmarksDetector\n",
        "\n",
        "def save2vid(filename, vid, frames_per_second):\n",
        "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "    torchvision.io.write_video(filename, vid, frames_per_second)\n",
        "\n",
        "def preprocess_video(src_filename, dst_filename):\n",
        "    landmarks = landmarks_detector(src_filename)\n",
        "    data = dataloader.load_data(src_filename, landmarks)\n",
        "    fps = cv2.VideoCapture(src_filename).get(cv2.CAP_PROP_FPS)\n",
        "    save2vid(dst_filename, data, fps)\n",
        "    return\n",
        "\n",
        "dataloader = AVSRDataLoader(modality=\"video\", speed_rate=1, transform=False, detector=\"mediapipe\", convert_gray=False)\n",
        "landmarks_detector = LandmarksDetector()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NNIy-oKz_Qa"
      },
      "outputs": [],
      "source": [
        "preprocess_video(src_filename=\"data/clip.mp4\", dst_filename=\"/data/roi.mp4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIex7JeE4WHy"
      },
      "source": [
        "### Extract visual-only features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UIJdkqL2Jkq"
      },
      "outputs": [],
      "source": [
        "modality = \"video\"\n",
        "model_conf = \"models/LRS3_V_WER19.1/model.json\"\n",
        "model_path = \"models/LRS3_V_WER19.1/model.pth\"\n",
        "pipeline = InferencePipeline(modality, model_path, model_conf, face_track=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojyoRXEr5Ox9"
      },
      "source": [
        "[**Option 1**]. Extract features from the output of Conformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGSp1X4m3djL",
        "outputId": "e75b5e1e-14b2-41b2-d65f-5218878c386b"
      },
      "outputs": [],
      "source": [
        "features = pipeline.extract_features(\"/data/clip.mp4\")\n",
        "print(features.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBgkRekr5gXE"
      },
      "source": [
        "[**Option 2**]. Extract features from the output of ResNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVTOD8LL4zUv",
        "outputId": "b466a13b-0915-4b46-9330-8bcf3a3714d1"
      },
      "outputs": [],
      "source": [
        "features = pipeline.extract_features(\"/data/clip.mp4\", extract_resnet_feats=True)\n",
        "print(features.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "BOSON_API_KEY = \"bai-Diz6JrS6rquzG1HSby-07fYX0AEgNJrCXKx0n6qr8F06ACSz\"\n",
        "\n",
        "client = openai.Client(\n",
        "    api_key=BOSON_API_KEY,\n",
        "    base_url=\"https://hackathon.boson.ai/v1\"\n",
        ")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"Qwen3-32B-non-thinking-Hackathon\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"I got this text from a lipreading model. Please correct any misheard or miswritten or incoherent words given the context to make it sound like what the speaker likely said, without adding or removing any content.\"},\n",
        "        {\"role\": \"user\", \"content\": \"THE OTHER DAY I HEARD SOMEONE COMPARE TRUMP TO THE NEIGHBOR WHO KEEPS RUNNING HIS LEAF BLOWER OUTSIDE DOOR WINDOW EVERY MINUTE OF EVERY DAY\"}\n",
        "    ],\n",
        "    max_tokens=2048,\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".vsr (3.11.14)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
